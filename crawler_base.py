# crawler_base.py
import os
import glob
import time
import random
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import config
from crawler_core import DataProcessor
from state_manager import StateManager
from smart_extractor import SmartExtractor

class BaseCrawler:
    def __init__(self, group_name, source_name, checkpoint_key=None):
        self.group_name = group_name
        self.processor = DataProcessor(source_name)
        self.save_key = checkpoint_key if checkpoint_key else group_name
        self.state = StateManager(group_name)
        self.smart_engine = SmartExtractor()

    # [Helper] ë¡œê·¸ì— í•­ìƒ ì¶œì²˜ë¥¼ ë¶™ì—¬ì£¼ëŠ” í•¨ìˆ˜
    def log(self, message):
        print(f"[{self.processor.source_name}] {message}")

    def _get_headers(self):
        return {
            'User-Agent': random.choice(config.USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
        }

    def get_soup(self, url, params=None):
        try:
            resp = requests.get(url, params=params, headers=self._get_headers(), timeout=10)
            if resp.status_code != 200: return None
            return BeautifulSoup(resp.text, 'html.parser')
        except Exception as e:
            self.log(f"ì ‘ì† ì—ëŸ¬: {e}")
            return None

    def post_json(self, url, data=None, params=None):
        try:
            resp = requests.post(url, data=data, params=params, headers=self._get_headers(), timeout=10)
            if resp.status_code != 200: return None
            return resp.json()
        except Exception:
            return None

    def fetch_parallel(self, items, worker_func, max_workers=10):
        results = []
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = executor.map(worker_func, items)
            for res in futures:
                if res: results.append(res)
        return results

    def fetch_items(self, page):
        raise NotImplementedError

    def run(self):
        self.log(f"ìˆ˜ì§‘ ì‹œì‘ (Page/Skip ê¸°ë°˜)")
        
        saved_page = self.state.load_checkpoint()
        current_page = saved_page if saved_page > 0 else 1
        
        while True:
            self.log(f"{current_page} í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")
            
            try:
                items = self.fetch_items(current_page)
                
                if not items:
                    self.log("ë°ì´í„° ì—†ìŒ. ìˆ˜ì§‘ ì¢…ë£Œ.")
                    self.state.reset_checkpoint()
                    break

                batch_data = []
                for item in items:
                    raw_comp = item.pop("ê¸°ì—…ëª…", "") 
                    raw_ceo = item.pop("ëŒ€í‘œìëª…", "")
                    
                    record = self.processor.create_record(raw_comp, raw_ceo, item)
                    
                    if self.state.is_new_or_changed(record['ê³ ìœ í‚¤'], record):
                        batch_data.append(record)

                if batch_data:
                    self.log(f"ìŠ¤ë§ˆíŠ¸ ì—”ì§„ ì‘ë™ ({len(batch_data)}ê±´ ë¶„ì„ ì¤‘...)")
                    enhanced_data = self.smart_engine.process_batch(batch_data, max_workers=10)
                    # send_to_gas ë‚´ë¶€ì—ì„œ "GAS ì „ì†¡ ì™„ë£Œ" ì¶œë ¥í•¨
                    self.processor.send_to_gas(enhanced_data)
                else:
                    self.log("ë³€ê²½ì‚¬í•­ ì—†ìŒ (Skip)")
                
                self.state.save_checkpoint(current_page + 1)
                current_page += 1
                time.sleep(random.uniform(1, 2))

            except Exception as e:
                self.log(f"ì—ëŸ¬ ë°œìƒ: {e}")
                break

class BaseFileCrawler(BaseCrawler):
    def __init__(self, group_name, source_name, target_folder=None):
        """
        :param group_name: ìƒíƒœ ê´€ë¦¬ë¥¼ ìœ„í•œ ê·¸ë£¹ëª… (DBíŒŒì¼ ì´ë¦„ ë“±)
        :param source_name: GAS ì „ì†¡ ì‹œ í‘œê¸°í•  ì¶œì²˜ëª…
        :param target_folder: 'FilesToParse' ì•„ë˜ì— ìœ„ì¹˜í•  í´ë”ëª… (ì˜ˆ: 'localdata_hospital')
        """
        self.group_name = group_name
        self.processor = DataProcessor(source_name)
        self.state = StateManager(group_name)
        self.smart_engine = SmartExtractor()
        folder_name = target_folder if target_folder else group_name
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •: FilesToParse/{target_folder}
        self.base_dir = os.path.join("FilesToParse", folder_name)

    def log(self, message):
        print(f"[{self.processor.source_name}] {message}")

    def process_file(self, file_path):
        """ê°œë³„ íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ë¡œì§ (ìì‹ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError

    def run(self):
        self.log(f"íŒŒì¼ íŒŒì‹± ëª¨ë“œ ì‹œì‘ (í´ë”: {self.base_dir})")

        # 1. í´ë” í™•ì¸
        if not os.path.exists(self.base_dir):
            os.makedirs(self.base_dir)
            self.log(f"í´ë”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {self.base_dir}")
            self.log("ì´ í´ë”ì— ë¶„ì„í•  íŒŒì¼ë“¤ì„ ë„£ê³  ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return

        # 2. íŒŒì¼ ëª©ë¡ ë¡œë“œ (ì´ë¦„ìˆœ ì •ë ¬ í•„ìˆ˜ - ìˆœì„œ ë³´ì¥ ìœ„í•´)
        # xml íŒŒì¼ë¿ë§Œ ì•„ë‹ˆë¼ í•„ìš”í•˜ë‹¤ë©´ ë‹¤ë¥¸ í™•ì¥ìë„ ì²˜ë¦¬ ê°€ëŠ¥í•˜ê²Œ glob ì‚¬ìš©
        files = sorted(glob.glob(os.path.join(self.base_dir, "*.*")))
        if not files:
            self.log("í´ë”ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 3. ì²´í¬í¬ì¸íŠ¸ í™•ì¸ (ë§ˆì§€ë§‰ìœ¼ë¡œ ì™„ë£Œí•œ íŒŒì¼ëª…)
        last_done_file = self.state.load_checkpoint()
        if last_done_file == 0:
            last_done_file = "" # ë¬¸ìì—´ ë¹„êµë¥¼ ìœ„í•´ ì´ˆê¸°í™”
        
        skip_mode = True if last_done_file else False
        
        self.log(f"ì´ {len(files)}ê°œ íŒŒì¼ ë°œê²¬.")
        if skip_mode:
            self.log(f"â–¶ ì´ì–´í•˜ê¸°: '{last_done_file}' ë‹¤ìŒ íŒŒì¼ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")

        # 4. íŒŒì¼ ìˆœíšŒ
        for file_path in files:
            file_name = os.path.basename(file_path)

            # ì´ì–´í•˜ê¸° ë¡œì§: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ê¹Œì§€ëŠ” ê±´ë„ˆëœ€
            if skip_mode:
                if file_name == last_done_file:
                    skip_mode = False # ì°¾ì•˜ë‹¤! ë‹¤ìŒ íŒŒì¼ë¶€í„° ì²˜ë¦¬
                continue

            self.log(f"ğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_name}")
            
            try:
                # ìì‹ í´ë˜ìŠ¤ì˜ process_file í˜¸ì¶œ
                self.process_single_file(file_path)
                
                # íŒŒì¼ í•˜ë‚˜ê°€ ëë‚  ë•Œë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (íŒŒì¼ëª…)
                self.state.save_checkpoint(file_name)
                
            except Exception as e:
                self.log(f"âŒ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ ({file_name}): {e}")
                # ì—ëŸ¬ ë°œìƒ ì‹œ ë©ˆì¶¤ (ë¬¸ì œ í•´ê²° í›„ ë‹¤ì‹œ ëŒë¦¬ê¸° ìœ„í•´)
                break
        
        self.log("ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ.")

    def process_single_file(self, file_path):
        # ìì‹ í´ë˜ìŠ¤ê°€ êµ¬í˜„í•˜ì§€ ì•Šì•˜ì„ ê²½ìš° ëŒ€ë¹„
        raise NotImplementedError("process_single_file ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")